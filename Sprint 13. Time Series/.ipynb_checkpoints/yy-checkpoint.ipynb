{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-description\" data-toc-modified-id=\"Data-description-1\">Data description</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\">Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-2.1\">Conclusion</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\">Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Trend\" data-toc-modified-id=\"Trend-3.1\">Trend</a></span></li><li><span><a href=\"#Seasonality\" data-toc-modified-id=\"Seasonality-3.2\">Seasonality</a></span></li><li><span><a href=\"#Residuals\" data-toc-modified-id=\"Residuals-3.3\">Residuals</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-4\">Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4.1\">Feature Engineering</a></span></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-4.2\">Hyperparameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-4.2.1\">Random Forest</a></span></li><li><span><a href=\"#XGBRegressor\" data-toc-modified-id=\"XGBRegressor-4.2.2\">XGBRegressor</a></span></li><li><span><a href=\"#LGBMRegressor\" data-toc-modified-id=\"LGBMRegressor-4.2.3\">LGBMRegressor</a></span></li><li><span><a href=\"#CatBoostRegressor\" data-toc-modified-id=\"CatBoostRegressor-4.2.4\">CatBoostRegressor</a></span></li><li><span><a href=\"#KNearest-Neighbors\" data-toc-modified-id=\"KNearest-Neighbors-4.2.5\">KNearest Neighbors</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-4.3\">Conclusion</a></span></li></ul></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-5\">Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-5.1\">Conclusion</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project description\n",
    "\n",
    "Sweet Lift Taxi company has collected historical data on taxi orders at airports. To attract more drivers during peak hours, we need to predict the amount of taxi orders for the next hour. Build a model for such a prediction.\n",
    "\n",
    "The RMSE metric on the test set should not be more than 48.\n",
    "\n",
    "\n",
    "## Data description\n",
    "\n",
    "The data is stored in file `taxi.csv`. The number of orders is in the '*num_orders*' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and numpy for data preprocessing and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# matplotlib and seaborn for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import module for splitting and cross-validation using gridsearch\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "# import metric to measure quality of model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# import time series split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# import statistics models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# import machine learning models\n",
    "from sklearn.linear_model import LinearRegression # import linear regression algorithm\n",
    "from sklearn.ensemble import RandomForestRegressor # import random forest algorithm\n",
    "from catboost import CatBoostRegressor, Pool # import catboost regressor\n",
    "from lightgbm import LGBMRegressor # import lightgbm regressor\n",
    "from xgboost import XGBRegressor # import xgboost regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/taxi.csv',index_col=[0], parse_dates=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine if columns in file have null values\n",
    "def get_percent_of_na(df,num):\n",
    "    df_nulls = pd.DataFrame(df.isna().sum(),columns=['Missing Values'])\n",
    "    df_nulls['Percent of Nulls'] = round(df_nulls['Missing Values'] / df.shape[0],num) *100\n",
    "    return df_nulls\n",
    "        \n",
    "# function to display general information about the dataset\n",
    "def get_info(df):\n",
    "    \"\"\"\n",
    "    This function uses the head(), info(), describe(), shape() and duplicated() \n",
    "    methods to display the general information about the dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\033[1m\" + '-'*100 + \"\\033[0m\")\n",
    "    print('Head:')\n",
    "    print()\n",
    "    display(df.head())\n",
    "    print('-'*100)\n",
    "    print('Info:')\n",
    "    print()\n",
    "    display(df.info())\n",
    "    print('-'*100)\n",
    "    print('Describe:')\n",
    "    print()\n",
    "    display(df.describe())\n",
    "    print('-'*100)\n",
    "    display(df.describe)\n",
    "    print()\n",
    "    print('Columns with nulls:')\n",
    "    display(get_percent_of_na(df, 4))  # check this out\n",
    "    print('-'*100)\n",
    "    print('Shape:')\n",
    "    print(df.shape)\n",
    "    print('-'*100)\n",
    "    print('Duplicated:')\n",
    "    print(\"\\033[1m\" + 'We have {} duplicated rows.\\n'.format(df.duplicated().sum()) + \"\\033[0m\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study the general information about the dataset \n",
    "print('General information about the dataframe')\n",
    "get_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_orders'] = df['num_orders'].astype('int32')\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# display index\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head of sorted data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the dates and times are in chronological order\n",
    "print(df.index.is_monotonic)\n",
    "print()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum value of index\n",
    "print('Minimum timestamp', df.index.min())\n",
    "print()\n",
    "# maximum value of index\n",
    "print('Maximum timestamp', df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize time series\n",
    "ts = df['num_orders']\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Number of taxi orders for Sweet Lift Taxi Company')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('No. of Orders')\n",
    "plt.plot(ts);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is a time series plot of the number of taxi orders for Sweet Lift Taxi Company between 1st March, 2018 and 31st August, 2018. Looking at the plot, we can see a trend in our data. This means that we can use a time series to model the data and generate forecasts. We can analyze the data using different components of a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consist of 26496 rows and 2 columns. There are no missing values or duplicated rows. The datetime column needs to be converted to the datetime datatype and the num_orders column needs to be converted to int32 in order to reduce memory requirements during computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resample data by 1 hour\n",
    "ts = ts.resample('1H').sum()\n",
    "ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataWeek = df['2018-03-01':'2018-03-14']\n",
    "dataWeek.plot(figsize=(10,3),title='2 weeks plot from 1st-March to 14th-March ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataWeek = dataWeek.resample('1H').sum()\n",
    "dataWeek['rolling_mean'] = dataWeek.rolling(30).mean() \n",
    "dataWeek.plot(figsize=(10,3),title='Rolling Mean from Mar 1st to Mar 14th');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "dataWeek['std'] = dataWeek['num_orders'].rolling(30).std() \n",
    "dataWeek.plot(figsize=(10,3),title='Rolling STD from Mar 1st to Mar 14th');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "ts_ = ts.resample('1D').sum()\n",
    "\n",
    "decomposed = seasonal_decompose(ts_)\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.subplot(311)\n",
    "decomposed.trend.plot(ax=plt.gca(), figsize=(15, 10))\n",
    "plt.title('Trend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the data shows upward trend. Using the trend line, we can make forecast into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonality\n",
    "plt.subplot(312)\n",
    "decomposed.seasonal.plot(ax=plt.gca(), figsize=(15, 10))\n",
    "plt.title('Seasonality');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataWeek = df['2018-03-01':'2018-03-14']\n",
    "tmp = dataWeek['2018-03-01':'2018-03-02']\n",
    "\n",
    "decomposed_hour = seasonal_decompose(tmp['num_orders'].dropna())\n",
    "decomposed_hour.seasonal.plot(figsize=(10,3), title='Hourly Seasonal Decomposition over 2 Days');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The plots above shows the periodic fluctuation in the time series within a certain period. These fluctuations form a pattern that tends to repeat from one seasonal period to the next one. The taxi rides start decreasing after midnight.\n",
    "They start increasing at 6am. They peak around noon, then before midnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Residuals\n",
    "plt.subplot(313)\n",
    "decomposed.resid.plot(ax=plt.gca(), figsize=(15, 10))\n",
    "plt.title('Residuals')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make new features \n",
    "def make_features(data, max_lag, rolling_mean_size):\n",
    "    data['month'] = data.index.month\n",
    "    data['day'] = data.index.day\n",
    "    data['dayofweek'] = data.index.dayofweek\n",
    "    data['hour'] = data.index.hour\n",
    "    \n",
    "    for lag in range(1, max_lag + 1):\n",
    "        data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
    "\n",
    "    data['rolling_mean'] = data['num_orders'].shift().rolling(rolling_mean_size).mean()\n",
    "# make new features \n",
    "ts = pd.DataFrame(ts)\n",
    "make_features(ts, 6, 7)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs from the time series data\n",
    "ts = ts.dropna()\n",
    "print('The time series has', ts.shape[0], 'rows and', ts.shape[1], 'features')\n",
    "print()\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# split the data into train and testing sets\n",
    "train, test = train_test_split(ts, shuffle=False, test_size=0.1)\n",
    "print(train.index.min(), train.index.max())\n",
    "print(test.index.min(), test.index.max())\n",
    "print()\n",
    "\n",
    "print('The train set has', train.shape[0], 'rows and', train.shape[1], 'features')\n",
    "print('The test set has', test.shape[0], 'rows and', test.shape[1], 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables for target and features\n",
    "features_train = train.drop(['num_orders'], axis=1)\n",
    "target_train = train['num_orders']\n",
    "\n",
    "features_test = test.drop(['num_orders'], axis=1)\n",
    "target_test = test['num_orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "print(tscv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LinearRegression()\n",
    "model.fit(features_train, target_train)\n",
    "predictions_test = model.predict(features_test)\n",
    "predictions_train = model.predict(features_train)\n",
    "print(\"MAE for the training set:\", mean_absolute_error(predictions_train, target_train))\n",
    "print(\"MAE for the test set: \", mean_absolute_error(predictions_test, target_test))\n",
    "print('Model RMSE for the training set:', mean_squared_error(predictions_train, target_train,squared=False))\n",
    "print('Model RMSE for the test set:', mean_squared_error(predictions_test, target_test,squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating pipelines.\n",
    "\n",
    "pipe_rfr = Pipeline([('scaler1', StandardScaler()),\n",
    "                    ('RandomForestRegressor', RandomForestRegressor(n_estimators=100))])\n",
    "\n",
    "pipe_linear = Pipeline([('scaler2', StandardScaler()),\n",
    "                       ('LinearRegression(Dummy)', LinearRegression())])\n",
    "\n",
    "pipe_cat_boost_r = Pipeline([('scaler3', StandardScaler()),\n",
    "                       ('CatBoostRegressor', CatBoostRegressor(verbose=500))])\n",
    "\n",
    "pipe_lgbm_r =  Pipeline([('scaler4', StandardScaler()),\n",
    "                       ('LGBMRegressor', LGBMRegressor())])\n",
    "\n",
    "pipe_xgb_r = Pipeline([('scaler5', StandardScaler()),\n",
    "                       ('XGBRegressor', XGBRegressor())])\n",
    "pipe_neighbors = Pipeline([('scaler6',StandardScaler()),('KNeighborsRegressor',KNeighborsRegressor())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of pipelines.\n",
    "pipelines = [pipe_rfr, pipe_linear, pipe_cat_boost_r, pipe_lgbm_r, pipe_xgb_r,pipe_neighbors]\n",
    "#Creating a dictionary of pipelines.\n",
    "pipe_dict = {pipe_rfr:'RandomForestRegressor', pipe_linear:'LinearRegression',\\\n",
    "             pipe_cat_boost_r: 'CatBoostRegressor', pipe_lgbm_r: 'LGBMRegressor', pipe_xgb_r:'XGBRegressor',pipe_neighbors:'KNeighborsRegressor'}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for pipe in pipelines:\n",
    "    print(pipe_dict[pipe])\n",
    "    print(cross_val_score(pipe, features_train, target_train, scoring='neg_root_mean_squared_error', cv=tscv))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creating a tree based model with best hyperparameters.\n",
    "rfr_param = {'n_estimators': (10, 25, 50, 100),\n",
    "              'max_depth': (None, 2, 4, 8, 10, 12),\n",
    "              } \n",
    "\n",
    "# Creating a grid model.\n",
    "RF_grid = GridSearchCV(RandomForestRegressor(random_state=0, criterion='mse'), param_grid=rfr_param, \n",
    "                       cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "RF_grid_model = RF_grid.fit(features_train, target_train)\n",
    "print(RF_grid_model.best_estimator_)\n",
    "print(RF_grid_model.best_score_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best hyperparameters are: {}'.format(RF_grid_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creating a gradient boosting descent model with best hyperparameters.\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "xgb_param = {'learning_rate': (0.001, 0.01, 0.1, 0.3),\n",
    "              'n_estimators': (10, 25, 50, 100),\n",
    "              'base_score': (0.25, 0.5, 0.75)\n",
    "              } \n",
    "\n",
    "# Creating a grid model.\n",
    "XGB_grid = GridSearchCV(XGBRegressor(), param_grid=xgb_param, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1) \n",
    "XGB_grid_model = XGB_grid.fit(features_train, target_train)\n",
    "print(XGB_grid_model.best_estimator_)\n",
    "print(XGB_grid_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best hyperparameters are: {}'.format(XGB_grid_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creating a gradient boosting descent model tuning best hyperparameters.\n",
    "lgbm_param = {'learning_rate': (0.001, 0.01, 0.05, 0.1),\n",
    "              'n_estimators': (50, 100,200,500),\n",
    "             'num_leaves': [5, 10, 20, 31]\n",
    "             } \n",
    "\n",
    "# Creating a grid model.\n",
    "LGBM_grid = GridSearchCV(LGBMRegressor(), param_grid=lgbm_param, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=1) \n",
    "LGBM_grid_model = LGBM_grid.fit(features_train, target_train)\n",
    "print(LGBM_grid_model.best_estimator_)\n",
    "print(LGBM_grid_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The best hyperparameters are: {}'.format(LGBM_grid_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creating a gradient boosting descent model tuning best hyperparameters.\n",
    "cat_param = {'learning_rate': [0.001, 0.01, 0.5],\n",
    "        'depth': [4, 6, 10]\n",
    "             \n",
    "       }\n",
    "\n",
    "# Creating a grid model.\n",
    "cat_grid = GridSearchCV(CatBoostRegressor(), param_grid=cat_param, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=1) \n",
    "cat_grid_model = cat_grid.fit(features_train, target_train)\n",
    "print(cat_grid_model.best_estimator_)\n",
    "print(cat_grid_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best hyperparameters are: {}'.format(cat_grid_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_param = {'n_neighbors' : range(1,5,1),\n",
    "            'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "# Creating a grid model.\n",
    "knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid=knn_param, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=1) \n",
    "knn_grid_model = knn_grid.fit(features_train, target_train)\n",
    "print(knn_grid_model.best_estimator_)\n",
    "print(knn_grid_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The best hyperparameters are: {}'.format(knn_grid_model.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this section, several different algorithms with various hyperparameters were trained.  observed the time it took to tune hyperparameters, train time and the model prediction time. The metric used to evaluate the model is the RMSE score. The KNeighbors regression algorithm had the fastest training time but had the worst RMSE score of -33.3. The LightGBM regressor had the best RMSE score of 23.64 and will be chosen for model testing for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LinearRegression Dummy\n",
    "dummy_test_model = Pipeline([('scaler0', StandardScaler()),\n",
    "                       ('LinearRegression(Dummy)', LinearRegression())])\n",
    "dummy_test_model.fit(features_train, target_train)\n",
    "dummy_predictions_test = dummy_test_model.predict(features_test)\n",
    "print('Model RMSE for the test set:', mean_squared_error(dummy_predictions_test, target_test,squared=False))\n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "lgbm_test_model = Pipeline([('scaler2', StandardScaler()),\n",
    "        ('LGBMRegressor', LGBMRegressor(learning_rate= 0.1, n_estimators= 200, num_leaves= 10))])\n",
    "lgbm_test_model.fit(features_train, target_train)\n",
    "lgbm_predictions_test = lgbm_test_model.predict(features_test)\n",
    "print('Model RMSE for the test set:', mean_squared_error(lgbm_predictions_test, target_test,squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the best RMSE score was chosen for model testing for this task. Using the LightGBM algorithm, we obtained an RMSE score of 44.08 for the test set. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
